# -*- coding: utf-8 -*-
"""LSTM for RR (V1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uPQWsqO4IEMY0J7zpQEkVQzOWdt8cojH

# Draft V2
"""

import pandas as pd
import numpy as np
import datetime
from itertools import combinations
from keras.utils import to_categorical
from keras.models import Sequential
from keras.layers import LSTM, RepeatVector, Dense, TimeDistributed
from sklearn.model_selection import KFold

def df_resampling(df, date_col, fault_col, date_opt):
  error_dummies=pd.get_dummies(df[fault_col])
  error_dummies['datetime']=df[date_col].tolist()
  error_dummies['datetime']=pd.to_datetime(error_dummies['datetime'])
  error_dummies.index = error_dummies['datetime']
  df_dummies=error_dummies.resample(date_opt).sum()
  return df_dummies

df_RR=pd.read_excel('/content/drive/My Drive/Colab Notebooks/RR Folder/New_RR.xlsx') 
df_RR['Date'] = df_RR['Msg Time'].apply(lambda x: x.date())
#df_RR['FM'] = df_RR['FM'].astype('category')
#df_RR['fm_cat'] = df_RR['FM'].cat.codes
seq_length = 24
predict_gap = 5
batch_size = 20

df_ac = df_RR[(df_RR['Engine']==1) &(df_RR['Source Type']=='EEC') & (df_RR['Side']==1.)] 


df_ac.FM = df_ac.FM.astype('category')
df_ac['fm_cat'] = df_ac.FM.cat.codes

"""### AKBAR's CODE"""

from keras.preprocessing import sequence
from keras.layers import Masking
#df_ac = df_RR.copy()
df_ac.Date = pd.to_datetime(df_ac.Date)
df_ac.index = df_ac.Date
df_ac.FM = df_ac.FM.astype('category')
df_ac['fm_cat'] = df_ac.FM.cat.codes

df_ac=df_ac.dropna(subset=['FM'])

lst_seq = []
for i in df_ac.AC.unique():
    #print('ac',i)
    for j in df_ac.loc[df_ac.AC==i,'Date'].unique():
        #print('date',j)
        lst_seq.append(list(df_ac.loc[(df_ac.AC==i)&(df_ac.Date == j), 'FM']))
        #lst_seq.append(list(df_ac.loc[(df_ac.AC==i)&(df_ac.Date == j), 'fm_cat']))

print('len of seq list', len(lst_seq))

lst_len = int(np.floor(np.mean(pd.Series([len(x) for x in  lst_seq]).value_counts().index[0:5])))
print('seq len',lst_len)

final_lst_seq=[]
for i in lst_seq:
    if(len(i) > lst_len):
        for j in range(0,len(i), lst_len):
            final_lst_seq.append(i[j:(j+lst_len)])
           
    else:
        final_lst_seq.append(i)

final_lst_seq = sequence.pad_sequences(final_lst_seq,maxlen=5, padding= 'post',value=999)

def generate_sequence(ac_lst, seq_length, predict_gap, rolling_step):  
  X_lst=[]
  y_lst=[]
 
  num_elements = len(ac_lst)
  start_point=(num_elements)%rolling_step
  for start in range(start_point, num_elements-seq_length-predict_gap, rolling_step):
      stop=start+seq_length
      X_lst.append(ac_lst[start:stop])
      y_lst.append(ac_lst[stop:stop+predict_gap])
  return X_lst, y_lst

from itertools import chain
X_train, y_train = generate_sequence(list(chain.from_iterable(final_lst_seq.tolist())), lst_len, lst_len,1)
X_train = np.array(X_train).reshape(-1,1,lst_len)
y_train = np.array(y_train).reshape(-1,1,lst_len)

srcTR_encoded = to_categorical(X_train, num_classes=e)
tarTR_encoded = to_categorical(y_train, num_classes=cardinality)
srcTR_encoded = srcTR_encoded[len(srcTR_encoded)%20:]
tarTR_encoded = tarTR_encoded[len(tarTR_encoded)%20:]

epoch =10
optimizer_opt = 'rmsprop'
n_in, n_out = lst_len, lst_len
tarTR_encoded = y_train
srcTR_encoded = X_train
print('uique FM', df_ac.FM.nunique())

from keras.layers import Bidirectional, Dropout
###
hidden_layer =  256 if int(srcTR_encoded.shape[0]/ (3 * (n_in+n_out))) < 256 else int(srcTR_encoded.shape[0]/ (3 * (n_in+n_out)))
from keras import callbacks
 

model = Sequential() 
model.add(Masking(mask_value=999,input_shape=(1, lst_len)))
model.add(Bidirectional(LSTM(hidden_layer, return_sequences=True)))
model.add(Dropout(0.2)) 
model.add(Bidirectional(LSTM(hidden_layer, return_sequences=True))) # try remove bidirecitonal 
model.add((Dense(lst_len, activation='softmax'))) # relu 
model.compile(loss='categorical_crossentropy', optimizer=optimizer_opt,\
              metrics = ['accuracy']) #metrics.Precision(), metrics.Recall()
       
earlyStop=callbacks.EarlyStopping(monitor="val_loss",verbose=1,mode='auto',patience=3)
trained_model = model.fit(srcTR_encoded, tarTR_encoded, epochs=epoch, \
          batch_size=batch_size, verbose=1, shuffle=True, validation_split=0.20, \
              callbacks=[earlyStop])

cardinality=len(df_ac['fm_cat'].unique())

df_ac.columns

cardinality

seq_length = 24
X_train = []
y_train = []

X_test = []
y_test = []

for ac in df_ac.AC.unique():
    ac_lst = df_ac[df_ac['AC']==ac]['fm_cat'].tolist()
    ac_train = ac_lst.copy() #[:int(len(ac_lst)*0.8)]
    #ac_test = ac_lst[int(len(ac_lst)*0.8):]
    
    sub_X, sub_y = generate_sequence(ac_train, seq_length, predict_gap, 1)
    X_train.extend(sub_X)
    y_train.extend(sub_y)


    #sub_X, sub_y = generate_sequence(ac_test, seq_length, predict_gap, 1)
    #X_test.extend(sub_X)
    #y_test.extend(sub_y)


#X_train = to_categorical(X_train, num_classes=cardinality)
X_train=np.array(X_train)
y_train = to_categorical(y_train, num_classes=cardinality)


#X_test = to_categorical(X_test, num_classes=cardinality)
#y_test = to_categorical(y_test, num_classes=cardinality)

X_train_new=np.zeros(shape=(X_train.shape[0], cardinality))

for row in range(len(X_train)):
  for fm in X_train[row]:
    X_train_new[row][fm]=1

from keras.layers import Embedding, Bidirectional, Dropout
epoch=20
hidden_layer =  256 if int(X_train.shape[0]/ (3 * (seq_length+predict_gap))) < 256 else int(X_train.shape[0]/ (3 * (seq_length+predict_gap)))
from keras import callbacks

model = Sequential() 
model.add(Embedding(64,10,input_length=cardinality))
model.add(Bidirectional(LSTM(hidden_layer, return_sequences=False)))
model.add(Dropout(0.2))

model.summary()

model.add(RepeatVector(5))
model.add(Bidirectional(LSTM(hidden_layer, return_sequences=True))) # try remove bidirecitonal 
model.add((Dense(y_train.shape[2], activation='softmax'))) # relu 
model.compile(loss='categorical_crossentropy', optimizer='sgd',\
              metrics = ['accuracy']) #metrics.Precision(), metrics.Recall()
       
earlyStop=callbacks.EarlyStopping(monitor="val_loss",verbose=1,mode='auto',patience=3)
trained_model = model.fit(X_train_new, y_train, epochs=epoch, \
          batch_size=batch_size, verbose=1, shuffle=True, validation_split=0.20, \
              callbacks=[earlyStop])







"""## Main """

df_RR['Flight Phase'].unique()

#df_RR=df_RR[df_RR['Flight Phase']!= False]

df_RR['Flight Phase']=df_RR['Flight Phase'].apply(lambda x: int(x.split('-')[0].strip()) if type(x)==str else 0)

AC_opt = df_RR.groupby('AC')['Date'].count()[df_RR.groupby('AC')['Date'].count()>1000].keys()
df_ac = df_RR[df_RR['AC'].isin(AC_opt)]
# Alternative II: spliting according to AC. 
test_ac = list(combinations(AC_opt,3))
train_ac=[list(np.setdiff1d(AC_opt, test_ac[x]))for x in range(len(test_ac))]
cardinality = len(df_RR['fm_cat'].unique())+1

df_Train[df_Train['AC'] == 6].sort_values(['Date','Flight Phase'])[['Date','Flight Phase','fm_cat']]

date_opt=[10.0, 18.0, 25.0, 31.0, 35.25, 41.0]

len_dic={}

for seq_len in date_opt:
  score_dic={}
  final_Tscore=[] 
  final_Fscore=[]
  final_Detail=[]

 

  for train, test in zip(train_ac[:5], test_ac[:5]):
    df_Train = df_RR[(df_RR['AC'].isin(train)) & ((df_RR['Source Type']=='EEC') \
                        & (df_RR['Side']==1.))]
    df_Test = df_RR[df_RR['AC'].isin(test)]
    X_train=[]
    y_train=[]
    for ac in df_Train['AC'].unique():
        ac_lst = df_Train[df_Train['AC']==ac].sort_values(['Date','Flight Phase'])['fm_cat'].tolist()
        sub_X, sub_y = generate_sequence(ac_lst, 30, 10)
        X_train.extend(sub_X)
        y_train.extend(sub_y)
        
    srcTR_encoded = to_categorical(X_train, num_classes=cardinality)
    tarTR_encoded = to_categorical(y_train, num_classes=cardinality)
    srcTR_encoded = srcTR_encoded[len(srcTR_encoded)%20:]
    tarTR_encoded = tarTR_encoded[len(tarTR_encoded)%20:]

    X_test=[]
    y_test=[]
    for ac in df_Test['AC'].unique():
      ac_lst = df_Test[df_Test['AC']==ac].sort_values('Date')['fm_cat'].tolist()
      sub_X, sub_y = generate_sequence(ac_lst, 30, 10)
      X_test.extend(sub_X)
      y_test.extend(sub_y)
          
    srcTE_encoded = to_categorical(X_test, num_classes=cardinality)
    tarTE_encoded = to_categorical(y_test, num_classes=cardinality)
    srcTE_encoded = srcTE_encoded[len(srcTE_encoded)%20:]
    tarTE_encoded = tarTE_encoded[len(tarTE_encoded)%20:]
    # define LSTM
    n_in = 30
    n_out = 10
    encoded_length = df_RR['fm_cat'].max()+3
    batch_size = 20

    # batch_size = len(X_train)

    model = Sequential() 
    model.add(LSTM(150, batch_input_shape=(batch_size, n_in, encoded_length), stateful=True))
    model.add(RepeatVector(n_out))
    model.add(LSTM(150, return_sequences=True, stateful=True))
    model.add(TimeDistributed(Dense(encoded_length, activation='softmax')))
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

    model.fit(srcTR_encoded, tarTR_encoded, epochs=20, \
              batch_size=batch_size, verbose=5, shuffle=False)

    output = model.predict(srcTE_encoded, batch_size=batch_size, verbose=0)

    
    detail_step=[]
    check_lst=[]
    for case in range(len(output)):
      indi_lst=[]
      for step in range(10):
        indi_lst.append(np.argmax(output[case][step]))
        #print(case,step,np.argmax(tarTE_encoded[case][step]),np.argmax(output[case][step]))
        detail_step.append(np.argmax(tarTE_encoded[case][step])==np.argmax(output[case][step]))
      # if able to predict more than one fualt type correctly without consider sequence
      check_lst.append(indi_lst)
      
    # this is for checking score for without considering sequence
    True_True=[]
    False_True=[]
    for ind in range(len(check_lst)):
      # % fault predicted correctly among all faults happening in next 10 steps
      True_True.append(len(set(y_test[ind]) & set(check_lst[ind]))/len(set(y_test[ind])))
      # Fault that is predicted but not happening in the next 10 step
      False_True.append(len([fault for fault in set(check_lst[ind]) if fault not in set(y_test[ind])])/10)
      
    # how percentage of cases that is able to correctly predict at least one fault that is happening at next 10 steps#
    final_Tscore.append(np.sum([1 for score in True_True if score>0.2])/len(True_True))
    final_Fscore.append(np.sum([1 for score in False_True if score!=0])/len(False_True)) 
    final_Detail.append(sum(detail_step)/len(detail_step))

    score_dic['final_Tscore'] = final_Tscore
    score_dic['final_Fscore'] = final_Fscore
    score_dic['final_Detail'] = final_Detail
  [str(seq_len)]=score_dic

import json
with open ('overall_score.json', 'w') as file:
  json.dump(len_dic,file)

"""#### sort by date output """

final_Tscore

final_Fscore

final_Detail

"""####

#### Sort by Date and Flight Phase
"""

final_Tscore

final_Fscore

final_Detail

"""## Alternative II"""



AC_opt = df_RR.groupby('AC')['Date'].count()[df_RR.groupby('AC')['Date'].count()>1000].keys()
df_ac = df_RR[df_RR['AC'].isin(AC_opt)]
# Alternative II: spliting according to AC. 
test_ac = list(combinations(AC_opt,3))
train_ac=[list(np.setdiff1d(AC_opt, test_ac[x]))for x in range(len(test_ac))]
cardinality = len(df_RR['fm_cat'].unique())+1

df_train=pd.DataFrame(columns=df_RR['fm_cat'].unique())
df_test=pd.DataFrame(columns=df_RR['fm_cat'].unique())
#for train_ind, test_ind in zip(train_ac, test_ac):
train_ind=train_ac[5]
test_ind=test_ac[5]
for ac in train_ind:
    df_TRfilt = df_RR[(df_RR['AC']==ac) & (df_RR['Source Type']=='EEC') \
                & (df_RR['Side']==1.)].sort_values(by='Date').reset_index(drop=True)
     
    df_TRfilt1 = df_resampling(df_TRfilt, 'Date', 'fm_cat', str(1)+'D')
    df_TRfilt2 = df_TRfilt1.applymap(lambda x: int(0) if x==0 else int(1))
    
    df_train = df_train.append(df_TRfilt2, 0)
    
for ac in test_ind:
    df_TEfilt = df_RR[(df_RR['AC']==ac) & (df_RR['Source Type']=='EEC') \
                & (df_RR['Side']==1.)].sort_values(by='Date').reset_index(drop=True)
     
    df_TEfilt1 = df_resampling(df_TEfilt, 'Date', 'fm_cat', str(1)+'D')
    df_TEfilt2 = df_TEfilt1.applymap(lambda x: int(0) if x==0 else int(1))
    
    df_test = df_test.append(df_TEfilt2, 0)

df_train = df_train.fillna(int(0))  
df_test = df_test.fillna(int(0))

input_days = 10
pred_days = 4
n_in = input_days
n_out = pred_days
batch_size = 20

X_train=[]
y_train=[]

num_elements=len(df_train)
for step in range(0, num_elements-n_in-n_out,n_in):
    X_train.append(df_train.iloc[step:step+n_in, :].values.tolist())
    y_train.append(df_train.iloc[step+n_in:step+n_in+n_out, :].values.tolist())

X_train = X_train[len(X_train)%20:]
y_train = y_train[len(y_train)%20:]

X_test=[]
y_test=[]

num_elements=len(df_test)
for step in range(0, num_elements-n_in-n_out,n_in):
    X_test.append(df_test.iloc[step:step+n_in, :].values.tolist())
    y_test.append(df_test.iloc[step+n_in:step+n_in+n_out, :].values.tolist())

X_test = X_test[len(X_test)%batch_size:]
y_test = y_test[len(y_test)%batch_size:]

encoded_length = np.array(X_train).shape[2]
pred_output = model_predict(n_in, n_out, 
              encoded_length,
              batch_size, np.array(X_train),
              np.array(y_train), np.array(X_test))

from sklearn.metrics import f1_score


output=pred_output.copy()
y_test=np.array(y_test)
detail_step=[]
true_true=[]
false_true=[]
empty_lst=[]
for case in range(len(output)):

  indi_lst=[]
  true_lst=[]
  for step in range(len(pred_output[0])):
    indi_lst.extend([ind for ind,val in enumerate(pred_output[case][step]) if val >0.3])
    true_lst.extend([ind for ind,val in enumerate(y_test[case][step]) if val==1])
 # print((indi_lst,true_lst))
    #print(case,step,np.argmax(tarTE_encoded[case][step]),np.argmax(output[case][step]))
    #detail_step.append([ind for ind,val in enumerate(y_test[case][step]) if val==1]==[ind for ind,val in enumerate(pred_output[case][step]) if val >0.1])
  # if able to predict more than one fualt type correctly without consider sequence
  
  if len(true_lst)!=0 and len(indi_lst)!=0:
    true_true.append(len(set(true_lst) & set(indi_lst))/ len(set(true_lst)))
    false_true.append(1-len(set(true_lst) & set(indi_lst))/len(set(indi_lst)))
  else:
    if len(indi_lst)==0:
      indi_lst.append(0)
    if len(true_lst)==0:
      true_lst.append(0)
    true_true.append(len(set(true_lst) & set(indi_lst))/ len(set(true_lst)))
    false_true.append(1-len(set(true_lst) & set(indi_lst))/len(set(indi_lst)))
    
print(np.sum([x for x in true_true if x>0.5])/len(true_true))
print(np.sum([x for x in true_true if x>0.3])/len(true_true))

AC_opt = df_RR.groupby('AC')['Date'].count()[df_RR.groupby('AC')['Date'].count()>1000].keys()
df_ac = df_RR[df_RR['AC'].isin(AC_opt)]
# Alternative II: spliting according to AC. 
test_ac = list(combinations(AC_opt,3))
train_ac=[list(np.setdiff1d(AC_opt, test_ac[x]))for x in range(len(test_ac))]
cardinality = len(df_RR['fm_cat'].unique())+1

df_train=pd.DataFrame(columns=df_RR['fm_cat'].unique())
df_test=pd.DataFrame(columns=df_RR['fm_cat'].unique())
#for train_ind, test_ind in zip(train_ac, test_ac):
train_ind=train_ac[4]
test_ind=test_ac[4]
for ac in train_ind:
    df_TRfilt = df_RR[(df_RR['AC']==ac) & (df_RR['Source Type']=='EEC') \
                & (df_RR['Side']==1.)].sort_values(by='Date').reset_index(drop=True)
     
    df_TRfilt1 = df_resampling(df_TRfilt, 'Date', 'fm_cat', str(1)+'D')
    df_TRfilt2 = df_TRfilt1.applymap(lambda x: int(0) if x==0 else int(1))
    
    df_train = df_train.append(df_TRfilt2, 0)
    
for ac in test_ind:
    df_TEfilt = df_RR[(df_RR['AC']==ac) & (df_RR['Source Type']=='EEC') \
                & (df_RR['Side']==1.)].sort_values(by='Date').reset_index(drop=True)
     
    df_TEfilt1 = df_resampling(df_TEfilt, 'Date', 'fm_cat', str(1)+'D')
    df_TEfilt2 = df_TEfilt1.applymap(lambda x: int(0) if x==0 else int(1))
    
    df_test = df_test.append(df_TEfilt2, 0)

df_train = df_train.fillna(int(0))  
df_test = df_test.fillna(int(0))   


X_train=[]
y_train=[]
X_test=[]
y_test=[]

num_elements=len(df_train)
for start, stop in zip(range(0, num_elements-2*n_out),\
                       range(n_in, num_elements-n_out)):
    X_train.append(df_train.iloc[start:stop, :].values.tolist())
    y_train.append(df_train.iloc[stop:stop+n_out, :].values.tolist())

X_train = X_train[len(X_train)%batch_size:]
y_train = y_train[len(y_train)%batch_size:]

num_elements=len(df_test)

for start, stop in zip(range(0, num_elements-2*n_out),\
                       range(n_in, num_elements-n_out)):
    X_test.append(df_test.iloc[start:stop, :].values.tolist())
    y_test.append(df_test.iloc[stop:stop+n_out, :].values.tolist())

X_test = X_test[len(X_test)%batch_size:]
y_test = y_test[len(y_test)%batch_size:]

encoded_length = np.array(X_train).shape[2]
pred_output = model_predict(n_in, n_out, 
              encoded_length,
              batch_size, np.array(X_train),
              np.array(y_train), np.array(X_test))

from sklearn.metrics import f1_score

output=pred_output.copy()
y_test=np.array(y_test)
detail_step=[]
true_true=[]
false_true=[]
empty_lst=[]

for case in range(len(output)):

  indi_lst=[]
  true_lst=[]
  for step in range(len(pred_output[0])):
    indi_lst.extend([ind for ind,val in enumerate(pred_output[case][step]) if val >0.3])
    true_lst.extend([ind for ind,val in enumerate(y_test[case][step]) if val==1])
  print((indi_lst,true_lst))
    #print(case,step,np.argmax(tarTE_encoded[case][step]),np.argmax(output[case][step]))
    #detail_step.append([ind for ind,val in enumerate(y_test[case][step]) if val==1]==[ind for ind,val in enumerate(pred_output[case][step]) if val >0.1])
  # if able to predict more than one fualt type correctly without consider sequence
  
  if len(true_lst)!=0 and len(indi_lst)!=0:
    true_true.append(len(set(true_lst) & set(indi_lst))/ len(set(true_lst)))
    false_true.append(1-len(set(true_lst) & set(indi_lst))/len(set(indi_lst)))
  else:
    if len(indi_lst)==0:
      indi_lst.append(0)
    if len(true_lst)==0:
      true_lst.append(0)
    true_true.append(len(set(true_lst) & set(indi_lst))/ len(set(true_lst)))
    false_true.append(1-len(set(true_lst) & set(indi_lst))/len(set(indi_lst)))
    
print(np.sum([x for x in true_true if x>0.5])/len(true_true))

print(np.sum([x for x in true_true if x>0.3])/len(true_true))
print(np.sum([x for x in true_true if x>0.5])/len(true_true))

"""## all ac of ONE SOURCE"""

## AC_opt = df_RR.groupby('AC')['Date'].count()[df_RR.groupby('AC')['Date'].count()>1000].keys()
AC_opt = df_RR.groupby('AC')['Date'].count()[df_RR.groupby('AC')['Date'].count()>800].keys()
df_ac = df_RR[df_RR['AC'].isin(AC_opt)]
# Alternative II: spliting according to AC. 
cardinality = len(df_RR['fm_cat'].unique())+1
df_train=pd.DataFrame(columns=df_RR['fm_cat'].unique())

#for train_ind, test_ind in zip(train_ac, test_ac):
df_ac = df_RR[df_RR['AC'].isin(AC_opt)]
# Alternative II: spliting according to AC. 

for ac in AC_opt:
    df_TRfilt = df_RR[(df_RR['AC']==ac) & (df_RR['Source Type']=='EEC') \
                & (df_RR['Side']==1.)].sort_values(by='Date').reset_index(drop=True)
    df_TRfilt = df_TRfilt.iloc[:int(len(df_TRfilt)*0.8),:]

    df_TRfilt1 = df_resampling(df_TRfilt, 'Date', 'fm_cat', str(1)+'D')
    df_TRfilt2 = df_TRfilt1.applymap(lambda x: int(0) if x==0 else int(1))
    
    df_train = df_train.append(df_TRfilt2, 0)
    

df_train = df_train.fillna(int(0))

df_test=pd.DataFrame(columns=df_RR['fm_cat'].unique())
for ac in AC_opt:
    df_TEfilt = df_RR[(df_RR['AC']==ac) & (df_RR['Source Type']=='EEC') \
                & (df_RR['Side']==1.)].sort_values(by='Date').reset_index(drop=True)
    
  
    df_TEfilt1 = df_resampling(df_TEfilt, 'Date', 'fm_cat', str(1)+'D')
    df_TEfilt2 = df_TEfilt1.applymap(lambda x: int(0) if x==0 else int(1))
    
    df_test = df_test.append(df_TEfilt2.iloc[-(i+14):-i,:], 0)

  
df_test = df_test.fillna(int(0))

for i in range(0,56,14):
  if i==0:
    df_test=pd.DataFrame(columns=df_RR['fm_cat'].unique())
    for ac in AC_opt:
        df_TEfilt = df_RR[(df_RR['AC']==ac) & (df_RR['Source Type']=='EEC') \
                    & (df_RR['Side']==1.)].sort_values(by='Date').reset_index(drop=True)
        
      
        df_TEfilt1 = df_resampling(df_TEfilt, 'Date', 'fm_cat', str(1)+'D')
        df_TEfilt2 = df_TEfilt1.applymap(lambda x: int(0) if x==0 else int(1))
        
        df_test = df_test.append(df_TEfilt1.iloc[-(i+14):,:], 0)

  else:
    df_test=pd.DataFrame(columns=df_RR['fm_cat'].unique())
    for ac in AC_opt:
        df_TEfilt = df_RR[(df_RR['AC']==ac) & (df_RR['Source Type']=='EEC') \
                    & (df_RR['Side']==1.)].sort_values(by='Date').reset_index(drop=True)
        
      
        df_TEfilt1 = df_resampling(df_TEfilt, 'Date', 'fm_cat', str(1)+'D')
        df_TEfilt2 = df_TEfilt1.applymap(lambda x: int(0) if x==0 else int(1))
        
        df_test = df_test.append(df_TEfilt1.iloc[-(i+14):-i,:], 0)

    
  df_test = df_test.fillna(int(0))   


  X_test=[]
  y_test=[]

  num_elements=len(df_test)
  for step in range(0, num_elements,n_in+n_out):
      X_test.append(df_test.iloc[step:step+n_in, :].values.tolist())
      y_test.append(df_test.iloc[step+n_in:step+n_in+n_out, :].values.tolist())

  X_test = X_test[len(X_test)%batch_size:]
  y_test = y_test[len(y_test)%batch_size:]

  encoded_length = np.array(X_train).shape[2]
  pred_output = model_predict(n_in, n_out, 
                encoded_length,
                batch_size, np.array(X_train),
                np.array(y_train), np.array(X_test))
  output=pred_output.copy()
  y_test=np.array(y_test)
  detail_step=[]
  pred_comp=[]
  true_comp=[]
  empty_lst=[]

  for case in range(len(output)):
    indi_lst=[]
    true_lst=[]

    for step in range(len(pred_output[0])):
      indi_lst.extend([ind, val for ind,val in enumerate(pred_output[case][step]) if val >0.3])
      true_lst.extend([ind, val for ind,val in enumerate(y_test[case][step]) if val==1])
      print(indi_lst,true_lst)
    pred_comp.extend(indi_lst)
    true_comp.extend(true_lst)

  print(len(set(true_comp)&set(pred_comp))/len(set(true_comp)))

set(pred_comp)

set(true_comp)



"""## Pomegranate """

! pip install pomegranate

def generate_sequence(ac_lst, seq_length, predict_gap, rolling_step):  
  X_lst=[]
  y_lst=[]
 
  num_elements = len(ac_lst)
  start_point=(num_elements)%rolling_step
  for start in range(start_point, num_elements-seq_length-predict_gap, rolling_step):
      stop=start+seq_length
      X_lst.append(ac_lst[start:stop])
      y_lst.append(ac_lst[stop:stop+predict_gap])
  return X_lst, y_lst

df_ac.columns

X_train = []
y_train = []

X_test = []
y_test = []

for ac in df_ac.AC.unique():
    ac_lst = df_RR[df_RR['AC']==ac].sort_values(['Date'])['fm_cat'].tolist()
    ac_train = ac_lst[:int(len(ac_lst)*0.8)]
    ac_test = ac_lst[:int(len(ac_lst)*0.8):]
    
    sub_X, sub_y = generate_sequence(ac_train, seq_length, predict_gap, 1)
    X_train.extend(sub_X)
    y_train.extend(sub_y)


    sub_X, sub_y = generate_sequence(ac_test, seq_length, predict_gap, 1)
    X_test.extend(sub_X)
    y_test.extend(sub_y)

df_bayes = pd.DataFrame(X_test)

def bayesNets_model(df_dummies, algo_opt):
    '''
    al_opt:
        [greedy exact-dp exact chow-liu]
    
    'chow-liu' is most time efficient. 
    but need to define the root node:For algorithms which require
    a single root (‘chow-liu’), this is the root for which all edges point away from.
    and the defualt is first column
    
    the result of exact-dp/exact and greedy are more close, but as well time-consuming
    ‘greedy’ that greedily attempts to find the best structure,
    and frequently can identify the optimal structure. 
    
    ‘exact’ to find the optimal Bayesian network, 
    and ‘exact-dp’ tries to find the shortest path on the entire order lattice, 
    no big difference beween exact and exact-dp    
    
    '''
    #tic = time.time()
    model = BayesianNetwork.from_samples(df_dummies, algorithm=algo_opt) 
    #t1 = time.time() - tic
    return model

from pomegranate import BayesianNetwork

model = bayesNets_model(df_bayes, 'chow-liu')

def visualize_dependency(error_lst, model):
    G = nx.Graph()
    G.add_node(error_lst[x] for x in range(len(error_lst))) 
    ## exact_dp algorithm
    
    for ind,state in enumerate(model.structure):
      try:
          for node in state:
              G.add_edge(error_lst[ind],error_lst[node])
      except:
          print('no dependency relation')
    return G

# To learn about the sturcture of dependency model
print(model.structure)
# To visualize the fault dependency

G=visualize_dependency(state_lst, model)
nx.draw_networkx(G, pos=nx.circular_layout(G), node_color='r', edge_color='b')

! pip install bayesloop

import scipy

scipy.__version__

